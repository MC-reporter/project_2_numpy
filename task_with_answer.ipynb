{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In this project, you have two tasks:\n",
        "1) To learn the manipulation and processing of image, then write codes to answer 4 questions.<br>\n",
        "2) To train a deep neural network to classify 10 classes of natural images. <br>\n",
        "The model architecture, training and validation dataset have been provided. <br>\n",
        "You are NOT allowed to modify the architecture of the model, but are permitted to amplify the dataset including but not limited to the manipulation methods we introduce in 1). <br>\n",
        "The metric is the classification ACCURACY that your submitted model can achieve on the TEST dataset.\n",
        "\n",
        "Hint: \n",
        "1) the test dataset in 2) contains the same 10 classes as the training dataset, of which images may be \n",
        "transformed by filpping, rotation, gaussian blur and etc. <br>\n",
        "\n",
        "2) Your submited model should be a .json file. After saving your model in .pth by running train.ipynb, you should convert the .pth file to be a .json file by executing the function pthtojson() in the convertmodel.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Preliminary: The following section is an example to show how to read and show an image from the file system. <mark>\n",
        "Hint: Use the image processing library Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # plt for displaying pictures\n",
        "import matplotlib.image as mpimg # mpimg for reading images\n",
        "import numpy as np\n",
        "\n",
        "img = mpimg.imread('images/source/n01532829/n01532829_110.JPEG')   # Read an image from the training dataset.\n",
        "# At this point 'img' is already an np.array and can be manipulated in any way\n",
        "# Visualize the image\n",
        "plt.imshow(img)  \n",
        "plt.show(img.all)\n",
        "type(img)\n",
        "# Show the shape of the image\n",
        "img.shape, img.dtype \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Based on the above operation, the image is transformed into the numpy array. Then, you can try to process the image using numpy package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Q1: Cut the following colored section of the figure and display it <mark>\n",
        "Hint: Use the index of numpy and plt.\n",
        "This is a given example for demonstrating the form of the answer. \n",
        "Then the following questions except for Q1 will ask you to fill the blank with only the illustrations of the desired images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to answer Q1\n",
        "def slice(img,left,right,top,bottom):\n",
        "    height,width,channel = img.shape \n",
        "    transformed_left = max(0,left)\n",
        "    transformed_right = min(width,right)\n",
        "    transformed_top = max(0,top)\n",
        "    transformed_bottom = min(height,bottom)\n",
        "    # Answer the question and write your code in here\n",
        "    img = img[transformed_left:transformed_right,transformed_top:transformed_bottom]\n",
        "    # End of the answer\n",
        "    return img\n",
        "slicedimg =slice(img,50,300,50,230)\n",
        "plt.imshow(slicedimg)\n",
        "plt.show(slicedimg.all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Q2: We use NumPy to separate the 3 color channels of the original image, including R, G, B. And please display the converted image. The R,G,B images are as follows:<br> ![alt R-channel](images/answer/rchannel.jpg)<br> ![alt R-channel](images/answer/gchannel.jpg)<br> ![alt R-channel](images/answer/bchannel.jpg)<mark>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill out the answer of Q2\n",
        "def splitChannel(img,channelIndex):\n",
        "    img_c = np.zeros(img.shape,dtype='uint8')\n",
        "    # Answer the question and write your code in here\n",
        "    img_c[:,:,channelIndex] = img[:,:,channelIndex] \n",
        "    # End of the answer\n",
        "    return img_c\n",
        "# Blue-channel\n",
        "B_img = splitChannel(img,2)\n",
        "# Green-channel\n",
        "G_img = splitChannel(img,1)\n",
        "# Red-channel\n",
        "R_img = splitChannel(img,0)\n",
        "plt.imshow(B_img)\n",
        "plt.show(B_img.all)\n",
        "plt.imshow(G_img)\n",
        "plt.show(G_img.all)\n",
        "plt.imshow(R_img)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Q3ï¼š According to the follow functions, please convert the RGB color image to grayscale images and display them <mark>\n",
        "We have provided six calculation methods of color to gray:\n",
        "- Traditional: GRAY[Y] <- 0.299 * R + 0.587 * G + 0.114 * B\n",
        "- Brightness priority conversion: GRAY[Y] <- (max(R,G,B) + min(R,G,B))/2\n",
        "- Average brightness conversion: GRAY[Y] <- (R + G + B) / 3\n",
        "- Weight Brightness Conversion: GRAY[Y] <- 0.21 * R + 0.72 * G + 0.07 * B\n",
        "- Maximum brightness conversion: GRAY[Y] <- max(R,G,B)\n",
        "- Minimum brightness conversion: GRAY[Y] <- min(R,G,B)\n",
        "\n",
        "<mark> <br>The gray images calculated by the above methods are as follows sequently:\n",
        "\n",
        "<br> ![alt R-channel](images/answer/type0_gray.jpg)<br> \n",
        " ![alt R-channel](images/answer/type1_gray.jpg)<br> \n",
        "  ![alt R-channel](images/answer/type2_gray.jpg)<br> \n",
        "   ![alt R-channel](images/answer/type3_gray.jpg)<br> \n",
        "    ![alt R-channel](images/answer/type4_gray.jpg)<br> \n",
        "    ![alt R-channel](images/answer/type5_gray.jpg)\n",
        "<mark> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill out the answer of Q3\n",
        "def RGBToGray(img,changeType=0):\n",
        "    imggray = np.zeros([img.shape[0],img.shape[1]], dtype = float)\n",
        "    # Answer the question and write your code in here\n",
        "    if changeType==0:\n",
        "        imggray = img[:,:,0] * 0.299 + img[:,:,1] * 0.587 + img[:,:,2] * 0.114\n",
        "    elif changeType==1:\n",
        "        imggray = np.max(img,axis=2)/2 + np.min(img,axis=2)/2\n",
        "    elif changeType==2:\n",
        "        imggray = np.mean(img,axis=2)\n",
        "    elif changeType==3:\n",
        "        imggray = img[:,:,0] * 0.21 + img[:,:,1] * 0.72 + img[:,:,2] * 0.07\n",
        "    elif changeType==4:\n",
        "        imggray = np.max(img,axis=2)\n",
        "    elif changeType==5:\n",
        "        imggray = np.min(img,axis=2)\n",
        "    else:\n",
        "        print(\"not implemented yet.\")\n",
        "    # End of the answer\n",
        "    return imggray\n",
        "\n",
        "imggray = RGBToGray(img,0)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type0_gray.jpg')\n",
        "\n",
        "imggray = RGBToGray(img,1)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type1_gray.jpg')\n",
        "\n",
        "imggray = RGBToGray(img,2)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type2_gray.jpg')\n",
        "\n",
        "imggray = RGBToGray(img,3)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type3_gray.jpg')\n",
        "imggray = RGBToGray(img,4)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type4_gray.jpg')\n",
        "\n",
        "imggray = RGBToGray(img,5)\n",
        "plt.imshow(imggray,cmap='gray')\n",
        "plt.savefig('images/answer/type5_gray.jpg')\n",
        "\n",
        "plt.show(imggray.all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Q4: You need to add the random noise to the original image and display it <mark>\n",
        "You can choose your own noise intensity and noise type\n",
        "\n",
        "<mark>An image added with 0.5 times noise that is sampled by np.random.randint within [0,255] is shown as<br>  ![alt](images/answer/noiseimage.jpg)<br> <mark>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill out the answer of Q4\n",
        "def addNoise(img,intensity=0.5):\n",
        "    # using np.random.randint to generate noise filled with random integers \n",
        "    noise =  np.random.randint(low=0,high=256, size = img.shape)\n",
        "    # scale the noise with the intensity, note you need to maintain the dtype of numpy array as integers\n",
        "    noise = np.array(noise*intensity,dtype=np.int8)\n",
        "    # clip the perturbed image to make sure its pixel values are within the valid range [0,255]\n",
        "    imgnoise = np.clip(img + noise,0,255)\n",
        "    # end of the answer\n",
        "    return imgnoise,noise\n",
        "imgnoise,noise = addNoise(img,intensity=0.5)\n",
        "plt.imshow(imgnoise)\n",
        "plt.savefig('images/answer/noiseimage.jpg')\n",
        "plt.show(imgnoise.all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <mark> Q5: Flip up and down, flip left and right, and rotate 180 degrees on the original image, e.g. \n",
        "<br> Up and down:<br>![alt](images/answer/flip_updown.jpg)<br>\n",
        "Left and right:<br>![alt](images/answer/flip_leftright.jpg)<br>\n",
        "180 degree:<br>![alt](images/answer/flip_180.jpg)<br>\n",
        " <mark> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Fill out the answer of Q5\n",
        "def filp(img,ftype='updown'):\n",
        "    if ftype =='updown':\n",
        "        img_flip = img[::-1, :, :]\n",
        "    elif ftype =='leftright':\n",
        "        img_flip= img[:, ::-1, :]\n",
        "    elif ftype == '180':\n",
        "        img_flip = img[::-1, ::-1, :]\n",
        "    else:\n",
        "        img_flip = img\n",
        "    return img_flip\n",
        "img_flip = filp(img,ftype='updown')\n",
        "plt.imshow(img_flip)\n",
        "plt.savefig('images/answer/flip_updown.jpg')\n",
        "plt.show(img_flip.all)\n",
        "img_flip = filp(img,ftype='leftright')\n",
        "plt.imshow(img_flip)\n",
        "plt.savefig('images/answer/flip_leftright.jpg')\n",
        "plt.show(img_flip.all)\n",
        "img_flip = filp(img,ftype='180')\n",
        "plt.imshow(img_flip)\n",
        "plt.savefig('images/answer/flip_180.jpg')\n",
        "plt.show(img_flip.all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def augTrainData():\n",
        "    data_transform = {\n",
        "        \"source\": transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     ])}\n",
        "    train_dataset = datasets.ImageFolder(root=os.path.join('images', \"source\"),\n",
        "                                            transform=data_transform[\"source\"])\n",
        "    train_num = len(train_dataset)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                  batch_size=1, shuffle=False,\n",
        "                                                  num_workers=4)\n",
        "\n",
        "    json_path = './class_indices.json'\n",
        "    assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
        "\n",
        "    for Try in range(3):\n",
        "        with open(json_path, \"r\") as f:\n",
        "            class_indict = json.load(f)\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        idx = 0\n",
        "        for train_data in train_bar:\n",
        "            train_images, train_labels = train_data\n",
        "            ttype = np.random.randint(0,5)\n",
        "            train_images_np = train_images.numpy()[0].transpose(1,2,0)   \n",
        "            train_images_np = np.array(np.round(np.clip(train_images_np, 0, 1)*255),dtype=np.uint8)\n",
        "            # ttype = 2\n",
        "            if ttype==0:\n",
        "                height,width,channel = train_images_np.shape \n",
        "                trans = slice(train_images_np,int(width*0.1),int(width*0.8),int(height*0.1),int(height*0.8))\n",
        "            elif ttype==1:\n",
        "                trans = splitChannel(train_images_np,1)\n",
        "            elif ttype==2:\n",
        "                trans = RGBToGray(train_images_np,0)\n",
        "            elif ttype==3:\n",
        "                trans, noise = addNoise(train_images_np,intensity=0.5)\n",
        "            elif ttype==4:\n",
        "                trans = filp(train_images_np,'updown')\n",
        "            else:\n",
        "                trans = train_images_np\n",
        "            real_label = class_indict[str(int(train_labels[0]))]\n",
        "            transimg = np.array(np.round(np.clip(trans, 0, 255)),dtype=np.uint8)\n",
        "            transimgpth='images/train/{}/{}_{}_{}_t{}.JPEG'.format(real_label,real_label,idx,Try,ttype)\n",
        "            if ttype == 2:\n",
        "                mpimg.imsave(transimgpth,transimg,0,255,cmap='gray')\n",
        "            else:\n",
        "                mpimg.imsave(transimgpth,transimg,0,255)\n",
        "            idx += 1\n",
        "augTrainData()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('new38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "96b9fa700bd2b81dc088566709540c4896ca7076cdc713321cca3dd2729c182e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
